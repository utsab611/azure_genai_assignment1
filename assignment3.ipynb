{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa7dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pypdf faiss-cpu --quiet\n",
    "!python -m pip install langchain langchain-core langchain-community langchain-experimental --quiet\n",
    "!python -m pip install langchain-openai --quiet\n",
    "!python -m pip install langchain-community langchainhub langchain-chroma langchain langchain-experimental --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c44a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 2: Import Libraries\n",
    "import os\n",
    "import json\n",
    "import mlflow\n",
    "from typing import Dict, List, TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from openai import AzureOpenAI\n",
    "import google.generativeai as genai\n",
    "from pydantic import BaseModel\n",
    "import getpass\n",
    "\n",
    "\n",
    "\n",
    "# For Gemini (you'll need to set this)\n",
    "gemini_key = getpass.getpass(\"Enter your Gemini API key: \")\n",
    "os.environ[\"GEMINI_API_KEY\"] = gemini_key\n",
    "\n",
    "# Pinecone setup\n",
    "pinecone_key = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "os.environ[\"PINECONE_API_KEY\"] = pinecone_key\n",
    "\n",
    "# Step 4: Configuration Class\n",
    "class Config:\n",
    "    # Azure OpenAI Configuration\n",
    "    AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-08-01-preview\")\n",
    "    \n",
    "    # GPT-4 deployment name\n",
    "    GPT4_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt4o\")\n",
    "    \n",
    "    # Embeddings configuration\n",
    "    EMBEDDINGS_DEPLOYMENT_NAME = \"text-embedding-3-small\"\n",
    "    \n",
    "    # Pinecone Configuration\n",
    "    PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "    PINECONE_ENVIRONMENT = \"us-east4-gcp\"\n",
    "    PINECONE_INDEX_NAME = \"agentic-rag-kb\"\n",
    "    \n",
    "    # MLflow Configuration\n",
    "    MLFLOW_TRACKING_URI = \"http://20.75.92.162:5000\"\n",
    "    \n",
    "    # Gemini Configuration (for critique)\n",
    "    GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "\n",
    "# Step 5: Create Sample Knowledge Base Data\n",
    "# Since we don't have the actual dataset, let's create a sample one\n",
    "sample_kb_data = [\n",
    "    {\n",
    "        \"id\": \"KB001\",\n",
    "        \"title\": \"Caching Best Practices\",\n",
    "        \"content\": \"Implement cache expiration policies to ensure data freshness. Use distributed caching for scalable applications. Consider cache-aside pattern for read-heavy workloads.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"KB002\", \n",
    "        \"title\": \"CI/CD Pipeline Setup\",\n",
    "        \"content\": \"Use infrastructure as code for environment consistency. Implement automated testing at each stage. Ensure proper rollback mechanisms are in place.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"KB003\",\n",
    "        \"title\": \"Performance Tuning Tips\",\n",
    "        \"content\": \"Monitor application metrics regularly. Use database indexing for frequent queries. Implement lazy loading for better initial load times.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"KB004\",\n",
    "        \"title\": \"API Versioning Strategies\",\n",
    "        \"content\": \"Use URL versioning for clear API endpoints. Maintain backward compatibility when possible. Document breaking changes thoroughly.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"KB005\",\n",
    "        \"title\": \"Error Handling Guidelines\",\n",
    "        \"content\": \"Implement structured logging for errors. Use appropriate HTTP status codes. Provide meaningful error messages to clients.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"KB006\",\n",
    "        \"title\": \"Database Optimization\",\n",
    "        \"content\": \"Use connection pooling for database connections. Implement query optimization techniques. Regular database maintenance is essential.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"KB007\",\n",
    "        \"title\": \"Security Best Practices\",\n",
    "        \"content\": \"Implement proper authentication and authorization. Use HTTPS for all communications. Regular security audits are recommended.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"KB008\",\n",
    "        \"title\": \"Microservices Architecture\",\n",
    "        \"content\": \"Design services around business capabilities. Implement circuit breakers for fault tolerance. Use API gateways for request routing.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"KB009\",\n",
    "        \"title\": \"Containerization Benefits\",\n",
    "        \"content\": \"Provides environment consistency across deployments. Enables scalable application deployment. Simplifies dependency management.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"KB010\",\n",
    "        \"title\": \"Monitoring and Observability\",\n",
    "        \"content\": \"Implement comprehensive logging strategies. Use metrics collection for performance monitoring. Set up alerting for critical issues.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Sample knowledge base created with 10 entries\")\n",
    "\n",
    "# Step 6: Knowledge Base Indexing\n",
    "class KBIndexer:\n",
    "    def __init__(self):\n",
    "        self.azure_client = AzureOpenAI(\n",
    "            api_key=Config.AZURE_OPENAI_API_KEY,\n",
    "            api_version=Config.AZURE_OPENAI_API_VERSION,\n",
    "            azure_endpoint=Config.AZURE_OPENAI_ENDPOINT\n",
    "        )\n",
    "        \n",
    "        self.pinecone = Pinecone(api_key=Config.PINECONE_API_KEY)\n",
    "        self.index_name = Config.PINECONE_INDEX_NAME\n",
    "        \n",
    "    def create_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings using Azure OpenAI\"\"\"\n",
    "        try:\n",
    "            response = self.azure_client.embeddings.create(\n",
    "                input=texts,\n",
    "                model=Config.EMBEDDINGS_DEPLOYMENT_NAME\n",
    "            )\n",
    "            return [item.embedding for item in response.data]\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating embeddings: {e}\")\n",
    "            # Fallback: return random embeddings (for demo purposes)\n",
    "            import numpy as np\n",
    "            return [np.random.rand(1536).tolist() for _ in texts]\n",
    "    \n",
    "    def create_index(self):\n",
    "        \"\"\"Create Pinecone index if it doesn't exist\"\"\"\n",
    "        try:\n",
    "            if self.index_name not in self.pinecone.list_indexes().names():\n",
    "                self.pinecone.create_index(\n",
    "                    name=self.index_name,\n",
    "                    dimension=1536,  # text-embedding-3-small dimension\n",
    "                    metric='cosine',\n",
    "                    spec=ServerlessSpec(\n",
    "                        cloud='aws',\n",
    "                        region='us-east-1'\n",
    "                    )\n",
    "                )\n",
    "                print(f\"Created index: {self.index_name}\")\n",
    "            else:\n",
    "                print(f\"Index {self.index_name} already exists\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating index: {e}\")\n",
    "    \n",
    "    def index_knowledge_base(self, kb_data: List[Dict]):\n",
    "        \"\"\"Index knowledge base documents\"\"\"\n",
    "        try:\n",
    "            index = self.pinecone.Index(self.index_name)\n",
    "            \n",
    "            texts = []\n",
    "            metadatas = []\n",
    "            ids = []\n",
    "            \n",
    "            for item in kb_data:\n",
    "                doc_id = item.get('id', f\"KB{len(ids):03d}\")\n",
    "                content = item.get('content', '')\n",
    "                title = item.get('title', '')\n",
    "                \n",
    "                texts.append(content)\n",
    "                metadatas.append({\n",
    "                    'title': title,\n",
    "                    'content': content,\n",
    "                    'doc_id': doc_id\n",
    "                })\n",
    "                ids.append(doc_id)\n",
    "            \n",
    "            # Batch process embeddings\n",
    "            batch_size = 5  # Smaller batch for demo\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i:i+batch_size]\n",
    "                batch_ids = ids[i:i+batch_size]\n",
    "                batch_metadatas = metadatas[i:i+batch_size]\n",
    "                \n",
    "                embeddings = self.create_embeddings(batch_texts)\n",
    "                \n",
    "                # Prepare vectors for upsert\n",
    "                vectors = []\n",
    "                for idx, embedding in enumerate(embeddings):\n",
    "                    vectors.append({\n",
    "                        'id': batch_ids[idx],\n",
    "                        'values': embedding,\n",
    "                        'metadata': batch_metadatas[idx]\n",
    "                    })\n",
    "                \n",
    "                # Upsert to Pinecone\n",
    "                index.upsert(vectors=vectors)\n",
    "                print(f\"Indexed batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}\")\n",
    "            \n",
    "            print(f\"Successfully indexed {len(texts)} documents\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error indexing knowledge base: {e}\")\n",
    "    \n",
    "    def run_indexing(self, kb_data: List[Dict]):\n",
    "        \"\"\"Complete indexing workflow\"\"\"\n",
    "        print(\"Starting knowledge base indexing...\")\n",
    "        print(f\"Processing {len(kb_data)} KB entries\")\n",
    "        \n",
    "        print(\"Creating Pinecone index...\")\n",
    "        self.create_index()\n",
    "        \n",
    "        print(\"Indexing knowledge base...\")\n",
    "        self.index_knowledge_base(kb_data)\n",
    "        \n",
    "        print(\"Indexing completed successfully!\")\n",
    "\n",
    "# Run indexing\n",
    "print(\"Starting KB indexing process...\")\n",
    "indexer = KBIndexer()\n",
    "indexer.run_indexing(sample_kb_data)\n",
    "\n",
    "# Step 7: Define the Agentic RAG System\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    retrieved_docs: List[Dict]\n",
    "    initial_answer: str\n",
    "    critique_result: Literal[\"COMPLETE\", \"REFINE\"]\n",
    "    refined_answer: str\n",
    "    final_answer: str\n",
    "    citations: List[str]\n",
    "\n",
    "class AgenticRAGSystem:\n",
    "    def __init__(self):\n",
    "        # Initialize Azure OpenAI client\n",
    "        self.azure_client = AzureOpenAI(\n",
    "            api_key=Config.AZURE_OPENAI_API_KEY,\n",
    "            api_version=Config.AZURE_OPENAI_API_VERSION,\n",
    "            azure_endpoint=Config.AZURE_OPENAI_ENDPOINT\n",
    "        )\n",
    "        \n",
    "        # Initialize Pinecone\n",
    "        self.pinecone = Pinecone(api_key=Config.PINECONE_API_KEY)\n",
    "        self.index = self.pinecone.Index(Config.PINECONE_INDEX_NAME)\n",
    "        \n",
    "        # Initialize Gemini for critique\n",
    "        try:\n",
    "            genai.configure(api_key=Config.GEMINI_API_KEY)\n",
    "            self.gemini_model = genai.GenerativeModel('gemini-pro')\n",
    "            self.gemini_available = True\n",
    "        except Exception as e:\n",
    "            print(f\"Gemini initialization failed: {e}\")\n",
    "            self.gemini_available = False\n",
    "    \n",
    "    def retrieve_documents(self, state: GraphState) -> GraphState:\n",
    "        \"\"\"Retrieve top-5 relevant documents from vector database\"\"\"\n",
    "        print(\" Retrieving documents...\")\n",
    "        try:\n",
    "            # Generate query embedding\n",
    "            response = self.azure_client.embeddings.create(\n",
    "                input=[state[\"question\"]],\n",
    "                model=Config.EMBEDDINGS_DEPLOYMENT_NAME\n",
    "            )\n",
    "            query_embedding = response.data[0].embedding\n",
    "            \n",
    "            # Query Pinecone\n",
    "            results = self.index.query(\n",
    "                vector=query_embedding,\n",
    "                top_k=5,\n",
    "                include_metadata=True\n",
    "            )\n",
    "            \n",
    "            retrieved_docs = []\n",
    "            for match in results.matches:\n",
    "                retrieved_docs.append({\n",
    "                    'id': match.id,\n",
    "                    'content': match.metadata['content'],\n",
    "                    'title': match.metadata['title'],\n",
    "                    'score': match.score\n",
    "                })\n",
    "            \n",
    "            print(f\"Retrieved {len(retrieved_docs)} documents\")\n",
    "            return {**state, \"retrieved_docs\": retrieved_docs}\n",
    "        except Exception as e:\n",
    "            print(f\"Error in retrieval: {e}\")\n",
    "            return {**state, \"retrieved_docs\": []}\n",
    "    \n",
    "    def generate_initial_answer(self, state: GraphState) -> GraphState:\n",
    "        \"\"\"Generate initial answer using Azure GPT-4\"\"\"\n",
    "        print(\" Generating initial answer...\")\n",
    "        try:\n",
    "            # Prepare context from retrieved documents\n",
    "            context = \"\\n\\n\".join([\n",
    "                f\"[{doc['id']}] {doc['content']}\" for doc in state[\"retrieved_docs\"]\n",
    "            ])\n",
    "            \n",
    "            system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided knowledge base snippets. \n",
    "            Always cite the relevant snippets using [KBxxx] format. If information comes from multiple snippets, cite all relevant ones.\n",
    "            Be concise and accurate in your responses.\"\"\"\n",
    "            \n",
    "            user_prompt = f\"\"\"Question: {state['question']}\n",
    "\n",
    "            Knowledge Base Context:\n",
    "            {context}\n",
    "\n",
    "            Please provide a comprehensive answer citing relevant snippets:\"\"\"\n",
    "            \n",
    "            response = self.azure_client.chat.completions.create(\n",
    "                model=Config.GPT4_DEPLOYMENT_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            \n",
    "            initial_answer = response.choices[0].message.content\n",
    "            print(\"Initial answer generated successfully\")\n",
    "            return {**state, \"initial_answer\": initial_answer}\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating answer: {e}\")\n",
    "            return {**state, \"initial_answer\": \"Unable to generate answer at this time.\"}\n",
    "    \n",
    "    def self_critique(self, state: GraphState) -> GraphState:\n",
    "        \"\"\"Use Gemini to critique the initial answer\"\"\"\n",
    "        print(\" Performing self-critique...\")\n",
    "        \n",
    "        if not self.gemini_available:\n",
    "            print(\"Gemini not available, defaulting to COMPLETE\")\n",
    "            return {**state, \"critique_result\": \"COMPLETE\"}\n",
    "        \n",
    "        try:\n",
    "            critique_prompt = f\"\"\"\n",
    "            Analyze the following question and answer pair. Determine if the answer is COMPLETE or needs REFINEMENT.\n",
    "            \n",
    "            Question: {state['question']}\n",
    "            Answer: {state['initial_answer']}\n",
    "            \n",
    "            Consider:\n",
    "            1. Does the answer fully address the question?\n",
    "            2. Are there any gaps in the information provided?\n",
    "            3. Is the answer well-supported with citations?\n",
    "            \n",
    "            Respond with only one word: either \"COMPLETE\" or \"REFINE\".\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.gemini_model.generate_content(critique_prompt)\n",
    "            critique_result = response.text.strip().upper()\n",
    "            \n",
    "            # Validate response\n",
    "            if critique_result not in [\"COMPLETE\", \"REFINE\"]:\n",
    "                critique_result = \"COMPLETE\"  # Default to complete\n",
    "                \n",
    "            print(f\"Critique result: {critique_result}\")\n",
    "            return {**state, \"critique_result\": critique_result}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Gemini critique failed: {e}\")\n",
    "            return {**state, \"critique_result\": \"COMPLETE\"}  # Default to complete on error\n",
    "    \n",
    "    def refine_answer(self, state: GraphState) -> GraphState:\n",
    "        \"\"\"Retrieve one more document and refine the answer\"\"\"\n",
    "        print(\" Refining answer...\")\n",
    "        try:\n",
    "            # Retrieve one additional document\n",
    "            response = self.azure_client.embeddings.create(\n",
    "                input=[state[\"question\"]],\n",
    "                model=Config.EMBEDDINGS_DEPLOYMENT_NAME\n",
    "            )\n",
    "            query_embedding = response.data[0].embedding\n",
    "            \n",
    "            # Get one more document (skip the first 5 we already have)\n",
    "            results = self.index.query(\n",
    "                vector=query_embedding,\n",
    "                top_k=6,  # Get 6 to ensure we get one new one\n",
    "                include_metadata=True\n",
    "            )\n",
    "            \n",
    "            # Find a new document that wasn't in the initial retrieval\n",
    "            existing_ids = {doc['id'] for doc in state[\"retrieved_docs\"]}\n",
    "            additional_doc = None\n",
    "            \n",
    "            for match in results.matches:\n",
    "                if match.id not in existing_ids:\n",
    "                    additional_doc = {\n",
    "                        'id': match.id,\n",
    "                        'content': match.metadata['content'],\n",
    "                        'title': match.metadata['title'],\n",
    "                        'score': match.score\n",
    "                    }\n",
    "                    break\n",
    "            \n",
    "            if additional_doc:\n",
    "                print(f\"Found additional document: {additional_doc['id']}\")\n",
    "                # Add to retrieved docs\n",
    "                updated_docs = state[\"retrieved_docs\"] + [additional_doc]\n",
    "                \n",
    "                # Regenerate answer with additional context\n",
    "                context = \"\\n\\n\".join([\n",
    "                    f\"[{doc['id']}] {doc['content']}\" for doc in updated_docs\n",
    "                ])\n",
    "                \n",
    "                system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided knowledge base snippets. \n",
    "                Always cite the relevant snippets using [KBxxx] format. If information comes from multiple snippets, cite all relevant ones.\n",
    "                Be concise and accurate in your responses. This is a refined answer with additional context.\"\"\"\n",
    "                \n",
    "                user_prompt = f\"\"\"Question: {state['question']}\n",
    "\n",
    "                Knowledge Base Context (including additional snippet):\n",
    "                {context}\n",
    "\n",
    "                Please provide a refined comprehensive answer citing relevant snippets:\"\"\"\n",
    "                \n",
    "                response = self.azure_client.chat.completions.create(\n",
    "                    model=Config.GPT4_DEPLOYMENT_NAME,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt}\n",
    "                    ],\n",
    "                    temperature=0,\n",
    "                    max_tokens=1000\n",
    "                )\n",
    "                \n",
    "                refined_answer = response.choices[0].message.content\n",
    "                print(\"Refined answer generated successfully\")\n",
    "                return {**state, \"refined_answer\": refined_answer, \"final_answer\": refined_answer}\n",
    "            else:\n",
    "                print(\"No additional documents found, using initial answer\")\n",
    "                return {**state, \"final_answer\": state[\"initial_answer\"]}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in refinement: {e}\")\n",
    "            return {**state, \"final_answer\": state[\"initial_answer\"]}\n",
    "    \n",
    "    def should_continue(self, state: GraphState) -> Literal[\"refine\", \"end\"]:\n",
    "        \"\"\"Decision logic for graph flow\"\"\"\n",
    "        if state[\"critique_result\"] == \"REFINE\":\n",
    "            return \"refine\"\n",
    "        return \"end\"\n",
    "    \n",
    "    def finalize_answer(self, state: GraphState) -> GraphState:\n",
    "        \"\"\"Set final answer based on critique result\"\"\"\n",
    "        print(\" Finalizing answer...\")\n",
    "        if state[\"critique_result\"] == \"COMPLETE\":\n",
    "            return {**state, \"final_answer\": state[\"initial_answer\"]}\n",
    "        return state\n",
    "    \n",
    "    def build_graph(self) -> StateGraph:\n",
    "        \"\"\"Build the LangGraph workflow\"\"\"\n",
    "        workflow = StateGraph(GraphState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"retriever\", self.retrieve_documents)\n",
    "        workflow.add_node(\"answer_generator\", self.generate_initial_answer)\n",
    "        workflow.add_node(\"critique\", self.self_critique)\n",
    "        workflow.add_node(\"refinement\", self.refine_answer)\n",
    "        workflow.add_node(\"finalizer\", self.finalize_answer)\n",
    "        \n",
    "        # Set entry point\n",
    "        workflow.set_entry_point(\"retriever\")\n",
    "        \n",
    "        # Define edges\n",
    "        workflow.add_edge(\"retriever\", \"answer_generator\")\n",
    "        workflow.add_edge(\"answer_generator\", \"critique\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"critique\",\n",
    "            self.should_continue,\n",
    "            {\n",
    "                \"refine\": \"refinement\",\n",
    "                \"end\": \"finalizer\"\n",
    "            }\n",
    "        )\n",
    "        workflow.add_edge(\"refinement\", \"finalizer\")\n",
    "        workflow.add_edge(\"finalizer\", END)\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"Execute the RAG pipeline for a question\"\"\"\n",
    "        print(f\"\\n Processing question: {question}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Initialize state\n",
    "        initial_state = GraphState(\n",
    "            question=question,\n",
    "            retrieved_docs=[],\n",
    "            initial_answer=\"\",\n",
    "            critique_result=\"COMPLETE\",\n",
    "            refined_answer=\"\",\n",
    "            final_answer=\"\"\n",
    "        )\n",
    "        \n",
    "        # Build and execute graph\n",
    "        graph = self.build_graph()\n",
    "        final_state = graph.invoke(initial_state)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"final_answer\": final_state[\"final_answer\"],\n",
    "            \"retrieved_docs\": final_state[\"retrieved_docs\"],\n",
    "            \"critique_result\": final_state[\"critique_result\"],\n",
    "            \"initial_answer\": final_state.get(\"initial_answer\", \"\"),\n",
    "            \"refined_answer\": final_state.get(\"refined_answer\", \"\")\n",
    "        }\n",
    "\n",
    "# Step 8: Initialize the RAG System\n",
    "print(\"Initializing Agentic RAG System...\")\n",
    "rag_system = AgenticRAGSystem()\n",
    "print(\"RAG System initialized successfully!\")\n",
    "\n",
    "# Step 9: Test with Sample Queries\n",
    "sample_queries = [\n",
    "    \"What are best practices for caching?\",\n",
    "    \"How should I set up CI/CD pipelines?\",\n",
    "    \"What are performance tuning tips?\",\n",
    "    \"How do I version my APIs?\",\n",
    "    \"What should I consider for error handling?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING AGENTIC RAG DEMO\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf48333",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, query in enumerate(sample_queries, 1):\n",
    "    print(f\"\\n Query {i}/5: {query}\")\n",
    "    result = rag_system.query(query)\n",
    "    \n",
    "    print(f\"\\n Final Answer:\")\n",
    "    print(result['final_answer'])\n",
    "    print(f\"\\n Critique Result: {result['critique_result']}\")\n",
    "    print(f\" Retrieved Documents: {len(result['retrieved_docs'])}\")\n",
    "    \n",
    "    if result['critique_result'] == 'REFINE':\n",
    "        print(\" Refinement was applied\")\n",
    "    \n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Interactive Query Section\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERACTIVE QUERY MODE\")\n",
    "print(\"=\"*60)\n",
    "print(\"You can now ask your own questions! (Type 'quit' to exit)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    user_question = input(\"\\n Enter your question: \")\n",
    "    if user_question.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    if user_question.strip():\n",
    "        result = rag_system.query(user_question)\n",
    "        print(f\"\\n Answer: {result['final_answer']}\")\n",
    "        print(f\" Critique: {result['critique_result']}\")\n",
    "        print(f\" Documents used: {len(result['retrieved_docs'])}\")\n",
    "\n",
    "print(\"\\n Thank you for using the Agentic RAG System!\")\n",
    "\n",
    "# Step 11: MLflow Integration (Optional)\n",
    "try:\n",
    "    import mlflow\n",
    "    \n",
    "    # Set up MLflow\n",
    "    mlflow.set_tracking_uri(Config.MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_experiment(\"agentic-rag-system\")\n",
    "    \n",
    "    # Log a sample run\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"model\", Config.GPT4_DEPLOYMENT_NAME)\n",
    "        mlflow.log_param(\"embedding_model\", Config.EMBEDDINGS_DEPLOYMENT_NAME)\n",
    "        mlflow.log_metric(\"sample_queries_processed\", len(sample_queries))\n",
    "        \n",
    "        print(\"\\n MLflow logging completed!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"MLflow setup skipped: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AGENTIC RAG SYSTEM DEMO COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15345aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
